{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAvdNhGn3dpl9A407Naj14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beniguha/GenAI/blob/main/Agentic-AI-Experiments/pure_python_RAG_no_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai chromadb python-dotenv"
      ],
      "metadata": {
        "id": "_CWf87L41UJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "vr7NCp2Q1ZEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "load_dotenv()\n",
        "\n",
        "# Get the API key from environment variables\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "hlrb5y5V1d9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------------------------------\n",
        "# 1. ENV + CLIENT SETUP\n",
        "# --------------------------------------------------\n",
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "mZoVxBlv1ftK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------------------------------\n",
        "# 2. EMBEDDING FUNCTION\n",
        "# --------------------------------------------------\n",
        "def get_embedding(text: str) -> list[float]:\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n"
      ],
      "metadata": {
        "id": "WS8MNXet1hc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 3. VECTOR DATABASE SETUP (Chroma)\n",
        "# --------------------------------------------------\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"documents\"\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    \"LangChain is an orchestration framework for large language models.\",\n",
        "    \"RAG stands for Retrieval Augmented Generation.\",\n",
        "    \"Vector databases store embeddings for similarity search.\",\n",
        "    \"LLMs cannot access private data unless it is provided in the prompt.\"\n",
        "]\n",
        "\n",
        "for idx, doc in enumerate(documents):\n",
        "    collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[doc],\n",
        "        embeddings=[get_embedding(doc)]\n",
        "    )"
      ],
      "metadata": {
        "id": "4_ltX_Tu1jkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 4. RETRIEVAL FUNCTION\n",
        "# --------------------------------------------------\n",
        "def retrieve_context(query: str, k: int = 2) -> str:\n",
        "    query_embedding = get_embedding(query)\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=k\n",
        "    )\n",
        "\n",
        "    return \"\\n\".join(results[\"documents\"][0])\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. LLM ANSWER FUNCTION\n",
        "# --------------------------------------------------\n",
        "def answer_question(question: str) -> str:\n",
        "    context = retrieve_context(question)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert assistant.\n",
        "Answer the question using ONLY the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 6. RUN\n",
        "# --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    question = \"What is LangChain?\"\n",
        "    answer = answer_question(question)\n",
        "    print(\"\\nANSWER:\\n\", answer)\n"
      ],
      "metadata": {
        "id": "Hxf6tzfd1mCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}