{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVUoT3ijBgPOHw13aYLCDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beniguha/GenAI/blob/main/Agentic-AI-Experiments/pure_python_RAG_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCb8kvnh0Jtc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_openai langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "M2WTP7KJ0MpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. ENV SETUP\n",
        "# --------------------------------------------------\n",
        "load_dotenv()\n",
        "# Get the API key from environment variables\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "KbG43a_M0PEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------------------------------\n",
        "# 2. EMBEDDINGS + VECTOR STORE\n",
        "# --------------------------------------------------\n",
        "#embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    api_key=openai_api_key\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    \"LangChain is an orchestration framework for large language models.\",\n",
        "    \"RAG stands for Retrieval Augmented Generation.\",\n",
        "    \"Vector databases store embeddings for similarity search.\",\n",
        "    \"LLMs cannot access private data unless it is provided in the prompt.\"\n",
        "]\n",
        "\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts=documents,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"documents\"\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n"
      ],
      "metadata": {
        "id": "1Yj7BwJW0SUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 3. PROMPT TEMPLATE\n",
        "# --------------------------------------------------\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an expert assistant.\n",
        "Answer the question using ONLY the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. LLM\n",
        "# --------------------------------------------------\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=openai_api_key\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. RAG FUNCTION\n",
        "# --------------------------------------------------\n",
        "def answer_question(question: str) -> str:\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    return response.content\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 6. RUN\n",
        "# --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    question = \"What is LangChain?\"\n",
        "    answer = answer_question(question)\n",
        "    print(\"\\nANSWER:\\n\", answer)\n"
      ],
      "metadata": {
        "id": "4vKdB2hF0Vbt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}